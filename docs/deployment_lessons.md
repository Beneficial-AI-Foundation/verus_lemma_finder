# Deployment Lessons Learned: Verus Lemma Finder Web Demo

**Context**: Deploying a Python web app with ML/semantic search (FastAPI + sentence-transformers)  
**Result**: Successfully deployed to Fly.io after trying 3 platforms

---

**TL;DR**: After 2+ hours trying 3 platforms, **Fly.io with Docker won**. Railway's "magic" auto-detection wasted 90 minutes with cryptic errors. Render's free tier had insufficient memory. Fly.io with an explicit Dockerfile worked in 15 minutes.

**Winner**: Fly.io + Docker  
**Live Demo**: https://verus-lemma-finder.fly.dev/

---

## ðŸ“Š Platform Comparison

| Platform | Time | Outcome | Memory | Cost | Ease | Would Recommend? |
|----------|------|---------|--------|------|------|------------------|
| **Render** | 20 min | OOM crash | 512MB | $0/$7 | Medium | Only with paid plan |
| **Railway** | 90 min | Build failures | 1GB+ | $5 credit | Hard | âŒ No |
| **Fly.io** | 15 min | âœ… Success | 1GB | Free* | Easy | âœ… **Yes!** |

*Requires credit card but usage is free within generous limits

---

## 1ï¸âƒ£ Render: Memory Issues

### What We Tried:
- Blueprint deployment with `render.yaml`
- Auto-detection of Python project
- Build command: `pip install -e ".[demo]"`

### What Happened:
```
âœ“ Build successful
âœ“ Starting deployment
âœ“ Installing dependencies
âœ“ Loaded 304 lemmas
âŒ Out of memory (used over 512Mi)
```

### Root Cause:
**Free tier only has 512MB RAM**
- Sentence transformers model: ~100MB
- PyTorch dependencies: ~50MB
- Model loading + embeddings: ~150MB+
- Total: Exceeds 512MB limit

### Solutions Tried:
1. âŒ Disable embeddings (defeats the purpose - semantic search is the main feature!)
2. âŒ Use lighter model (would reduce quality)
3. âœ… **Upgrade to Starter plan ($7/mo)** - Would work but not free

### Verdict:
**Render works well BUT requires paid plan for ML apps**

### When to Use Render:
- âœ… Production deployments (worth $7/mo)
- âœ… Apps without heavy ML dependencies
- âŒ Free ML/AI demos

---

## 2ï¸âƒ£ Railway

### What We Tried:
1. **Nixpacks with `nixpacks.toml`** (30 min) - Failed
2. **Railway.toml config** (15 min) - Failed
3. **Nixpacks with `nixpacks.json`** (15 min) - Failed
4. **Pure auto-detection** (20 min) - Built but timed out
5. **Docker approach** (10 min) - Would have worked if we'd tried it first!

### The Problem:
Railway uses **Nixpacks** to auto-generate Dockerfiles. It kept generating malformed files:

```dockerfile
# Generated by Nixpacks (BROKEN!)
...
24 |     RUN  python -m pip install --upgrade pip
25 | >>> pip install -e ".[demo]"  # âŒ Missing RUN command!
26 |
```

**Error**: `dockerfile parse error on line 25: unknown instruction: pip`

### Why It Failed:
1. **Mysterious Build Phase**: Nixpacks auto-added a "build" phase with commands from `render.yaml`
2. **Config Conflicts**: Our `nixpacks.toml`, `railway.toml`, and `render.yaml` conflicted
3. **Opaque Errors**: Hard to debug what Nixpacks was actually doing
4. **Ignored Configs**: Sometimes our config files were completely ignored

### Attempts to Fix:

#### Attempt 1: nixpacks.toml
```toml
[phases.setup]
nixPkgs = ["python312", "gcc"]

[phases.install]
cmds = ["pip install -e '.[demo]'"]

[start]
cmd = "uvicorn demo.server:app --host 0.0.0.0 --port $PORT"
```
**Result**: Still generated broken build phase âŒ

#### Attempt 2: .railwayignore
Added `render.yaml` to ignore list
**Result**: Still picked up commands from somewhere âŒ

#### Attempt 3: nixpacks.json (different format)
Switched from TOML to JSON hoping for better parsing
**Result**: Same error âŒ

#### Attempt 4: Remove ALL config
Let Railway auto-detect everything
**Result**: Built successfully but **timed out** during deployment âŒ

### What We Learned:
1. **Nixpacks is unpredictable** - "Magic" doesn't always work
2. **Debugging is painful** - Can't see what Nixpacks generates until it fails
3. **Config conflicts are common** - Multiple config files interact in unexpected ways
4. **Auto-detection has quirks** - Looks at `uv.lock`, `pyproject.toml`, other files
5. **Docker would have been simpler** - We should have tried it first!

### Verdict:
**Railway wasted 90 minutes with cryptic errors. Not recommended for complex Python projects.**

### When to Use Railway:
- âœ… Simple Node.js apps
- âœ… Projects where Nixpacks "just works"
- âŒ Complex Python with ML dependencies
- âŒ When you need debugging transparency

---

## 3ï¸âƒ£ Fly.io: Success! âœ…

### What We Tried:
1. Used **explicit Dockerfile** (no auto-generation)
2. Fixed one issue (`Readme.md` missing from COPY)
3. Deployed successfully

### The Dockerfile:
```dockerfile
# Multi-stage build for minimal image size
FROM python:3.12-slim as builder

WORKDIR /app

# Install uv (fast Python package installer)
RUN pip install --no-cache-dir uv

# Copy dependency files (including Readme.md - important!)
COPY pyproject.toml Readme.md ./
COPY src/ ./src/

# Install dependencies
RUN uv pip install --system -e ".[demo]"

# Production stage
FROM python:3.12-slim

WORKDIR /app

# Copy installed packages from builder
COPY --from=builder /usr/local/lib/python3.12/site-packages /usr/local/lib/python3.12/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin

# Copy application code
COPY src/ ./src/
COPY demo/ ./demo/
COPY data/ ./data/

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV PORT=8000

# Run the application
CMD uvicorn demo.server:app --host 0.0.0.0 --port $PORT
```

### Deployment Process:
```bash
# 1. Install Fly CLI
curl -L https://fly.io/install.sh | sh

# 2. Login
flyctl auth login

# 3. Launch (auto-detects Dockerfile)
flyctl launch --name verus-lemma-finder

# 4. Deploy
flyctl deploy
```

### What Happened:
```
âœ“ Building image (5 minutes)
âœ“ Pushed to registry (30 seconds)
âœ“ Deployed to 2 machines (high availability)
âœ“ Started successfully
âœ“ Loaded 304 lemmas
âœ“ Loaded embeddings: (304, 384)
âœ“ Model loaded
âœ“ Ready with 1 project(s)
âœ“ Uvicorn running on http://0.0.0.0:8000

âœ… DEPLOYED: https://verus-lemma-finder.fly.dev/
```

### Why It Worked:
1. **Explicit Dockerfile** - We control every step
2. **Clear errors** - When Readme.md was missing, error was obvious
3. **Easy to debug** - Can test Docker locally: `docker build . && docker run`
4. **Transparent** - See exactly what's happening at each step
5. **Generous free tier** - 1GB RAM, good for ML apps

### Verdict:
**Fly.io + Docker = Winning combination. Simple, transparent, reliable.**

### When to Use Fly.io:
- âœ… **ML/AI apps** - Good memory allocation
- âœ… **Docker-based deployments** - Excellent support
- âœ… **Free tier demos** - Generous limits
- âœ… **Production apps** - Scales well
- âœ… **When you value debuggability** - Clear, helpful errors

---

## ðŸŽ“ Key Lessons Learned

### 1. Explicit Configuration > "Magic" Auto-Detection

**Explicit (Docker):**
```dockerfile
RUN pip install -e ".[demo]"
```
- âœ… You know exactly what happens
- âœ… Easy to debug
- âœ… Testable locally
- âœ… Portable across platforms

**Magic (Nixpacks):**
```
Auto-detecting... ðŸª„
Generating Dockerfile... ðŸŽ°
??? âŒ Build failed with cryptic error
```
- âŒ Don't know what it generated
- âŒ Hard to debug
- âŒ Platform-specific
- âŒ Unpredictable

**Lesson**: Sometimes "more work" (writing Dockerfile) is actually **less work** (faster to deploy).

---

### 2. Memory Requirements for ML Apps

**Minimum for semantic search with embeddings:**
- Base Python + FastAPI: ~50MB
- Sentence transformers: ~100MB
- PyTorch: ~50MB
- Model loading + embeddings: ~150MB
- **Total: ~350MB minimum**

**Platform Requirements:**
- âŒ 512MB: Too small (Render free)
- âœ… 1GB: Comfortable (Fly.io, Railway)
- âœ… 2GB+: Ideal (Render Starter)

**Lesson**: Check memory limits BEFORE choosing a platform for ML apps.

---

### 3. Clear Error Messages Save Hours

**Good Error (Fly.io):**
```
OSError: Readme file does not exist: Readme.md

hint: This usually indicates a problem with the package or the 
build environment.
```
- âœ… Clear problem
- âœ… Obvious fix
- âœ… 2 minutes to resolve

**Bad Error (Railway):**
```
dockerfile parse error on line 25: unknown instruction: pip
```
- âŒ What Dockerfile? (auto-generated, can't see it)
- âŒ Why is pip on its own line? (unknown)
- âŒ How to fix? (unclear)
- âŒ 90 minutes wasted

**Lesson**: Good error messages are worth their weight in gold.

---

### 4. Free Tier â‰  Best for Production

**Render Free Tier:**
- 512MB RAM
- Sleeps after 15 min
- Good for: Static sites, simple APIs
- Bad for: ML apps, always-on services

**Fly.io Free Tier:**
- 1GB RAM (with credit card)
- Stops after 5 min on trial (fixable with CC)
- Good for: ML demos, Docker apps
- Great for: Testing before production

**Railway Free Tier:**
- $5 credit/month
- Good memory
- Good for: Simple apps
- Bad for: Complex Python (Nixpacks issues)

---

### 5. Test Locally First

**With Docker (what worked):**
```bash
# Test locally before deploying
docker build -t lemma-finder .
docker run -p 8000:8000 lemma-finder

# If it works locally, it works in production
flyctl deploy
```

**With Nixpacks (what failed):**
```
# Can't test locally - Nixpacks generates Dockerfile on their servers
railway up

# If it fails, can't debug locally
# Have to guess and re-deploy
```

**Lesson**: If you can test locally, you'll debug 10x faster.

---

### 6. Don't Be Afraid of Docker

**Before this project (common misconception):**
> "Docker is complex! Let's use auto-detection to avoid it."

**After this project (reality):**
> "Docker is explicit and testable! Auto-detection wasted hours."

**Reality Check:**
- Writing a Dockerfile: **10 minutes**
- Debugging Nixpacks: **90 minutes**

**Lesson**: Docker isn't scary. It's actually simpler than magical auto-detection for complex apps.

---

## ðŸ“‹ Decision Framework for Future Deployments

### Choose **Render** if:
- âœ… You're willing to pay $7/mo
- âœ… You want simplicity with good support
- âœ… You don't need complex builds
- âŒ NOT for free ML demos

### Choose **Railway** if:
- âœ… You have a simple Node.js app
- âœ… Nixpacks works for your use case (test first!)
- âŒ NOT for complex Python/ML
- âŒ NOT if you value your time

### Choose **Fly.io** if:
- âœ… You're deploying an ML/AI app
- âœ… You want explicit control (Docker)
- âœ… You need good debugging
- âœ… You want generous free tier
- âœ… **This should be your default choice**

---

## ðŸš€ Recommended Deployment Path

For a Python ML/AI web app (like this one):

### Option A: Fast Path (Recommended)
```bash
1. Write a Dockerfile (10 min)
2. Test locally with Docker (5 min)
3. Deploy to Fly.io (5 min)

Total time: 20 minutes
Success rate: High
```

### Option B: The Path We Took (Don't Do This)
```bash
1. Try Render (20 min) â†’ OOM
2. Try Railway (90 min) â†’ Build errors
3. Try Fly.io (15 min) â†’ Success!

Total time: 125 minutes
Success rate: Eventually...
```

**Lesson**: We should have tried Fly.io + Docker first!

---

## ðŸ’° Cost Comparison (For Reference)

### Monthly Costs:

**Render:**
- Free: 0GB for ML (crashes)
- Starter: $7/mo for 512MB â†’ 2GB

**Railway:**
- Free: $5 credit (~750 hours)
- Pro: $20/mo

**Fly.io:**
- Free: $0 (with credit card, within limits)*
- Usage-based: Pay only for what you use

*3 shared-cpu-1x 256MB VMs + 160GB outbound transfer free

### For This Demo:
- Memory needed: ~350MB
- Fly.io free tier: Perfect âœ…
- Railway free tier: Would work (if Nixpacks cooperated)
- Render free tier: Insufficient âŒ

---


## ðŸŽ¯ Final Recommendations

### For This Project (Semantic Search Demo):
**âœ… Use Fly.io with Docker**
- Deployed: https://verus-lemma-finder.fly.dev/
- Works perfectly with full semantic search
- Free (with credit card)
- Easy to maintain

### For Similar ML/AI Projects:
**1. Fly.io** (best free option)
**2. Render Starter** (best paid option, $7/mo)
**3. AWS/GCP/Azure** (if you need enterprise features)
**4. Railway** (only as last resort, use Docker not Nixpacks)

### For Non-ML Projects:
- Simple sites: Vercel, Netlify, GitHub Pages
- Simple APIs: Railway (Nixpacks might work), Render
- Complex apps: Fly.io, Render, traditional hosting

---

## ðŸ”š Conclusion

**What Worked:**
- âœ… Fly.io + Docker
- âœ… Explicit configuration
- âœ… Local testing with Docker
- âœ… Clear error messages

**What Didn't:**
- âŒ Railway's Nixpacks auto-detection
- âŒ Render's free tier (insufficient memory)
- âŒ Trying "magic" solutions before explicit ones

**Time Breakdown:**
- Render: 20 minutes
- Railway: 90 minutes (wasted!)
- Fly.io: 15 minutes
- **Total**: 125 minutes
- **Could have been**: 20 minutes if we'd gone to Fly.io first

**Final Wisdom:**
> "Explicit is better than implicit."  
> â€” The Zen of Python

This applies to deployment too! Docker (explicit) beat Nixpacks (implicit) by being **simpler, faster, and more reliable**.

